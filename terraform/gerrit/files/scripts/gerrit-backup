#!/bin/bash -ex
tag=$(date +%Y%m%d)

function error() {
    echo "$@"
    exit 1
}

if [[ -z "${REGION}" || -z "${VOLUME_ID}" ]]
then
  error "REGION and VOLUME_ID must be present in the environment"
fi


# If an existing volume is present, we want to fail up front
echo "Checking to ensure no backup volume is present"
if [ "$(aws ec2 describe-volumes --region ${REGION} --filters Name=tag:Name,Values=gerrit-backup-volume | jq -r '.Volumes[0]')" != "null" ]
then
  error "Backup volume already exists"
fi

# We'll trap EXIT and clean up the volume if it's been created
function cleanup() {
    for volume in $@
    do
        echo "Cleaning up ${volume}"
        sleep 1
        umount /mnt/{backup,scratch} &>/dev/null || true
        aws ec2 detach-volume --volume-id ${volume} --force --region ${REGION} || true
        sleep 5
        aws ec2 wait volume-available --volume-ids ${volume} --region ${REGION}
        aws ec2 delete-volume --volume-id ${volume} --region ${REGION}
    done
}

echo "Creating volumes"
export snapshot=$(aws ec2 describe-snapshots --filters "Name=volume-id,Values=${VOLUME_ID}"  "Name=status,Values=completed" --region ${REGION}  | jq -r '.[]|max_by(.StartTime)|.SnapshotId')
export backup_volume=$(/usr/local/bin/aws ec2 create-volume \
                --region ${REGION} \
                --volume-type gp3 \
                --snapshot-id ${snapshot} \
                --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=gerrit-backup-volume},{Key=Owner,Value=build-team},{Key=Project,Value=gerrit},{Key=Purpose,Value=backup-restore}]' \
                --availability-zone ${REGION}a | jq -r '.VolumeId')
export scratch_volume=$(/usr/local/bin/aws ec2 create-volume \
                --region ${REGION} \
                --volume-type gp3 \
                --size 120 \
                --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=gerrit-scratch-volume},{Key=Owner,Value=build-team},{Key=Project,Value=gerrit},{Key=Purpose,Value=backup-restore}]' \
                --availability-zone ${REGION}a | jq -r '.VolumeId')

trap "cleanup ${backup_volume} ${scratch_volume}" EXIT

# Check volumes
if [[ "${backup_volume}" = "" || "${scratch_volume}" = "" ]]
then
    error "Volume creation failed"
fi

echo "Attaching volumes"
aws ec2 wait volume-available --volume-ids ${backup_volume} --region ${REGION}
aws ec2 wait volume-available --volume-ids ${scratch_volume} --region ${REGION}
aws ec2 attach-volume --volume-id ${backup_volume} --instance-id `cat /var/lib/cloud/data/instance-id` --device /dev/sdf --region ${REGION}
aws ec2 attach-volume --volume-id ${scratch_volume} --instance-id `cat /var/lib/cloud/data/instance-id` --device /dev/sdg --region ${REGION}
aws ec2 wait volume-in-use --volume-ids ${backup_volume} --region ${REGION}
aws ec2 wait volume-in-use --volume-ids ${scratch_volume} --region ${REGION}
sleep 10
sudo mkfs -t ext4 /dev/sdg

for vol in backup scratch
do
    if ! grep -qs "${vol}" /proc/mounts
    then
        sudo mount /mnt/${vol}
    fi
done

cd /mnt/backup

echo "Backing up"
if ! sudo tar -czf /mnt/scratch/backup-${tag}.tgz \
    "cache" \
    "data" \
    "db" \
    "etc" \
    "git" \
    "hooks" \
    "index" \
    "lib" \
    "logs" \
    "plugins" \
    "static"
then
    error "Couldn't create tarball"
else
    aws s3 cp /mnt/scratch/backup-${tag}.tgz s3://cb-gerrit.backups/ || \
        error "S3 upload failed"
fi

sudo rm -rf /mnt/scratch/backup-${tag}.tgz
